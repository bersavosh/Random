{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parallelization of independent instances\n",
    "\n",
    "The only package you need is [Joblib](https://pythonhosted.org/joblib/parallel.html), you can simply install it in conda with:\n",
    "\n",
    ">`conda install -c anaconda joblib `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from astropy.io import fits,ascii\n",
    "from astropy.time import Time\n",
    "\n",
    "# For parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get around the complication of parallel processing, pools, etc. is to have what you want to do each iteration defined as a function and then pass that function to Joblib to work with. So I define some dummy functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def galpy_stuff(theta,r,z):\n",
    "    # Dummy function, replace with needed functions, etc.\n",
    "    galpy_magic = np.random.rand()\n",
    "    return galpy_magic\n",
    "\n",
    "# The kick velocity you want\n",
    "V_kick = 200.0\n",
    "\n",
    "# Setting the random seed to allow reproducibility\n",
    "np.random.seed(seed = 48129)\n",
    "\n",
    "def simulate(i):\n",
    "    # \"i\" is just an iterator for parellization and has no other purpose\n",
    "    # Choosing a random direction for the kick:\n",
    "    #   a random number between 0 and 2pi for theta\n",
    "    V_kick_theta = (np.random.rand())*2*np.pi\n",
    "    #   a random number between 0 and +kick_v for r\n",
    "    V_kick_r = V_kick*np.random.rand()\n",
    "    #   kick in z is chosen so that Vr^2 + Vz^2 = V_kick\n",
    "    V_kick_z = np.sqrt(V_kick**2 - V_kick_r**2) \n",
    "\n",
    "    # Now you can add all the Galpy magic you want, as functions or otherwise\n",
    "    # I add a dummy function as an example:\n",
    "    galpy_results = galpy_stuff(V_kick_theta,V_kick_r,V_kick_z)\n",
    "    \n",
    "    # A simple counter so you can check how much is done.\n",
    "    if (i+1)%5000 == 0:\n",
    "        print 'Loop: Reaching iteration ',i+1\n",
    "\n",
    "    return galpy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below is all you need for parallelization of loops with independent iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available cores: 8\n",
      "Loop: Reaching iteration  5000\n",
      "Loop: Reaching iteration  10000\n",
      "Loop Done.\n"
     ]
    }
   ],
   "source": [
    "# Now with functions defined, we can run the process:\n",
    "\n",
    "# Your simulation sample size:\n",
    "sample_size = 10000\n",
    "\n",
    "# Counting the number of available cores and using all of them:\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print 'Number of available cores:', multiprocessing.cpu_count()\n",
    "\n",
    "# Passing the job to Joblib to run the loop in parallel on all cores:\n",
    "mc_results = Parallel(n_jobs=num_cores)(delayed(simulate)(i) for i in range(sample_size))\n",
    "print 'Loop Done.'\n",
    "\n",
    "# Save the output in pickle:\n",
    "pickle.dump(mc_results, open('kick_mcmc_v'+str(V_kick)+'_sample'+str(sample_size)+'_mjd'+str(Time.now().mjd)+'.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful to save the results of computationally expensive processes to an external file (as opposed to leave them in kernel memory) as soon as they are done so you don't lose or overwrite them. Specially helpful if you save them in a format that you can smoothly restore later on. So here I save the output in [`pickle`](https://docs.python.org/2/library/pickle.html) format (a python objects) and name the output file with values of `V_kick`, `sample_size` and the time when the simulation was finished to avoid overwriting. There are more intelligent ways to do this, this is just a quick way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To read a pickle file:\n",
    "mc_pickle = pickle.load(open('kick_mcmc_v200.0_sample10000_mjd58310.2365667.p','rb'))\n",
    "\n",
    "# you can continue your analysis with that later if things crash or you shut down the kernel.\n",
    "# The pickle does not change what you have:\n",
    "mc_pickle == mc_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
